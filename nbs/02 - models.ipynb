{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce74545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformers fork: https://github.com/AnswerDotAI/transformers/tree/cla-llama\n",
    "\n",
    "# vscode jupyter kill hanging process:\n",
    "#  ps aux | grep \"/workspace/py_venvs/cla/bin/python -m ipykernel_launcher\" | awk '{print $2}' | xargs kill -9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a8110fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import safetensors\n",
    "import safetensors.torch\n",
    "from safetensors.torch import save_file\n",
    "from transformers import AutoConfig\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b7ea3",
   "metadata": {},
   "source": [
    "### Prepare VLLM Weights (Full Finetune)\n",
    "\n",
    "These models are trained with different CLA factors, key-value states are shared between consequent layers. For example with CLA factor 2, with layer 0 and layer 1, layer 2 and layer 3, etc and with CLA factor 3, with (layer 0, layer 1, layer 2) and (layer 3, layer 4 and layer 5), etc.\n",
    "\n",
    "For fast evaluation benchmarking we can simply copy over the first layers' key-value weights and make it compatible with VLLM.\n",
    "\n",
    "In the actual efficient inference implementation, we will use the shared key-value states to reduce the memory footprint and KV cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/workspace/models/llama-3-8b-instruct-hqq-plus-dataset-CLA-3/model_state_dict.safetensors')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dir = Path(\"/workspace/models/\")\n",
    "model_path = \"llama-3-8b-instruct-hqq-plus-dataset-CLA-3\"\n",
    "list(Path(models_dir/model_path).glob(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b993a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = safetensors.torch.load_file(str(models_dir/model_path/'model_state_dict.safetensors'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c99d17e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cla_fixed_weights = {}\n",
    "cla_factor = 3\n",
    "for name, param in iter(weights.items()):\n",
    "    if \"k_proj\" in name or \"v_proj\" in name:\n",
    "        layer_idx = int(name.split('.')[2])\n",
    "        if layer_idx % cla_factor != 0:\n",
    "            cla_group_idx = layer_idx // cla_factor * cla_factor\n",
    "            cla_fixed_weights[name] = weights[name.replace(f\"model.layers.{layer_idx}\", f\"model.layers.{cla_group_idx}\")].clone()\n",
    "        else:\n",
    "            cla_fixed_weights[name] = weights[name]\n",
    "    else:\n",
    "        cla_fixed_weights[name] = weights[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "938bec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create save dir\n",
    "save_model_dir = models_dir/(model_path + \"-vllm\")\n",
    "os.makedirs(save_model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac883045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save fixed weights\n",
    "save_file(cla_fixed_weights, save_model_dir/\"model_state_dict.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model config\n",
    "model_config = AutoConfig.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "model_config.rope_scaling['type'] = 'dynamic'\n",
    "model_config.rope_scaling['factor'] = 2.0\n",
    "model_config_filename = save_model_dir/\"config.json\"\n",
    "with open(model_config_filename, \"w+\") as f: json.dump(model_config.to_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "148e5c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/workspace/models/llama-3-8b-instruct-hqq-plus-dataset-CLA-3-vllm/config.json'),\n",
       " PosixPath('/workspace/models/llama-3-8b-instruct-hqq-plus-dataset-CLA-3-vllm/model_state_dict.safetensors')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(save_model_dir.glob(\"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96591b94",
   "metadata": {},
   "source": [
    "### Test VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4300070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vllm\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510beeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-18 13:48:31 llm_engine.py:169] Initializing an LLM engine (v0.5.0.post1) with config: model='/workspace/models/llama-3-8b-instruct-hqq-plus-dataset-CLA-2', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/workspace/models/llama-3-8b-instruct-hqq-plus-dataset-CLA-2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-18 13:49:23 model_runner.py:234] Loading model weights took 14.9614 GB\n",
      "INFO 07-18 13:49:26 gpu_executor.py:83] # GPU blocks: 11714, # CPU blocks: 2048\n",
      "INFO 07-18 13:49:30 model_runner.py:864] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-18 13:49:30 model_runner.py:868] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-18 13:49:44 model_runner.py:1022] Graph capturing finished in 14 secs.\n"
     ]
    }
   ],
   "source": [
    "# FIXME: \"/workspace/models/llama-3-8b-instruct-hqq-plus-dataset-CLA-2-vllm\",\n",
    "# Copying over key-value weights doesn't work, model training might have went wrong.\n",
    "# Ideally key-value proj weights shouldn't be updated during training for the layers sharing key value states from previous layers.\n",
    "\n",
    "# Looks like kv weights didn't get updated for the expected layers but yet copying over doesn't work.\n",
    "# Does it mean kv weights weren't trained but still used for that layer? So layer sharing wasn't successful?\n",
    "\n",
    "# TODO: Copying weights won't work because hidden states passed to kv weights also change every layer, so the only way is to share the kv activations.\n",
    "\n",
    "llm = LLM(model=\"/workspace/models/llama-3-8b-instruct-hqq-plus-dataset-CLA-2\", \n",
    "          tokenizer=\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "          dtype=\"bfloat16\",\n",
    "          tensor_parallel_size=1,\n",
    "          enforce_eager=False, \n",
    "          gpu_memory_utilization=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8280d89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf2934f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it, est. speed input: 10.95 toks/s, output: 33.36 toks/s]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Say hello world 10 times as a numbered list\"\n",
    "# prompt = \"Write a poem about love and robots\"\n",
    "# prompt = \"Continue the fib series: 1 1 2 3 5 8\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs =llm.generate(prompt, SamplingParams(temperature=0.0, max_tokens=256))\n",
    "response_text = outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "807a00f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the list of \"Hello World\" said 10 times:\n",
      "\n",
      "1. Hello World\n",
      "2. Hello World\n",
      "3. Hello World\n",
      "4. Hello World\n",
      "5. Hello World\n",
      "6. Hello World\n",
      "7. Hello World\n",
      "8. Hello World\n",
      "9. Hello World\n",
      "10. Hello World\n"
     ]
    }
   ],
   "source": [
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1757fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check key value proj weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b0d07c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers.utils import hub, SAFE_WEIGHTS_NAME, SAFE_WEIGHTS_INDEX_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "idx = hub.cached_file(MODEL_NAME, SAFE_WEIGHTS_INDEX_NAME)\n",
    "pretrained_files, _ = hub.get_checkpoint_shard_files(MODEL_NAME, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "449c6b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_weights = {}\n",
    "for fn in pretrained_files:\n",
    "\torig_weights.update(safetensors.torch.load_file(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e11baf41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(orig_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLA2_weights = safetensors.torch.load_file(\"/workspace/models/llama-3-8b-instruct-hqq-plus-dataset-CLA-2/model_state_dict.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70796201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained: model.layers.0.self_attn.k_proj.weight 0.7948763370513916\n",
      "Trained: model.layers.0.self_attn.v_proj.weight 0.4667935371398926\n",
      "CLA KV shared: model.layers.1.self_attn.k_proj.weight 1.0\n",
      "CLA KV shared: model.layers.1.self_attn.v_proj.weight 1.0\n",
      "Trained: model.layers.2.self_attn.k_proj.weight 0.8456332683563232\n",
      "Trained: model.layers.2.self_attn.v_proj.weight 0.524782657623291\n",
      "CLA KV shared: model.layers.3.self_attn.k_proj.weight 1.0\n",
      "CLA KV shared: model.layers.3.self_attn.v_proj.weight 1.0\n",
      "Trained: model.layers.4.self_attn.k_proj.weight 0.8604786396026611\n",
      "Trained: model.layers.4.self_attn.v_proj.weight 0.6191747188568115\n",
      "CLA KV shared: model.layers.5.self_attn.k_proj.weight 1.0\n",
      "CLA KV shared: model.layers.5.self_attn.v_proj.weight 1.0\n",
      "Trained: model.layers.6.self_attn.k_proj.weight 0.871098518371582\n",
      "Trained: model.layers.6.self_attn.v_proj.weight 0.5928778648376465\n",
      "CLA KV shared: model.layers.7.self_attn.k_proj.weight 1.0\n",
      "CLA KV shared: model.layers.7.self_attn.v_proj.weight 1.0\n",
      "Trained: model.layers.8.self_attn.k_proj.weight 0.8642866611480713\n",
      "Trained: model.layers.8.self_attn.v_proj.weight 0.5908112525939941\n",
      "Trained: model.layers.10.self_attn.k_proj.weight 0.8647482395172119\n",
      "Trained: model.layers.10.self_attn.v_proj.weight 0.5814406871795654\n",
      "CLA KV shared: model.layers.11.self_attn.k_proj.weight 1.0\n",
      "CLA KV shared: model.layers.11.self_attn.v_proj.weight 1.0\n",
      "Trained: model.layers.12.self_attn.k_proj.weight 0.87143874168396\n",
      "Trained: model.layers.12.self_attn.v_proj.weight 0.6559610366821289\n",
      "CLA KV shared: model.layers.13.self_attn.k_proj.weight 1.0\n",
      "CLA KV shared: model.layers.13.self_attn.v_proj.weight 1.0\n",
      "Trained: model.layers.14.self_attn.k_proj.weight 0.8634459972381592\n",
      "Trained: model.layers.14.self_attn.v_proj.weight 0.6185848712921143\n",
      "CLA KV shared: model.layers.15.self_attn.k_proj.weight 1.0\n",
      "CLA KV shared: model.layers.15.self_attn.v_proj.weight 1.0\n",
      "Trained: model.layers.16.self_attn.k_proj.weight 0.8693246841430664\n",
      "Trained: model.layers.16.self_attn.v_proj.weight 0.6260638236999512\n",
      "CLA KV shared: model.layers.17.self_attn.k_proj.weight 1.0\n",
      "CLA KV shared: model.layers.17.self_attn.v_proj.weight 1.0\n",
      "Trained: model.layers.18.self_attn.k_proj.weight 0.8760559558868408\n",
      "Trained: model.layers.18.self_attn.v_proj.weight 0.6240475177764893\n",
      "CLA KV shared: model.layers.19.self_attn.k_proj.weight 1.0\n",
      "CLA KV shared: model.layers.19.self_attn.v_proj.weight 1.0\n",
      "Trained: model.layers.20.self_attn.k_proj.weight 0.8707470893859863\n",
      "Trained: model.layers.20.self_attn.v_proj.weight 0.6603317260742188\n",
      "CLA KV shared: model.layers.9.self_attn.k_proj.weight 1.0\n",
      "CLA KV shared: model.layers.9.self_attn.v_proj.weight 1.0\n",
      "CLA KV shared: model.layers.21.self_attn.k_proj.weight 1.0\n",
      "CLA KV shared: model.layers.21.self_attn.v_proj.weight 1.0\n",
      "Trained: model.layers.22.self_attn.k_proj.weight 0.8720059394836426\n",
      "Trained: model.layers.22.self_attn.v_proj.weight 0.6900904178619385\n",
      "CLA KV shared: model.layers.23.self_attn.k_proj.weight 1.0\n",
      "CLA KV shared: model.layers.23.self_attn.v_proj.weight 1.0\n",
      "Trained: model.layers.24.self_attn.k_proj.weight 0.8591570854187012\n",
      "Trained: model.layers.24.self_attn.v_proj.weight 0.7248454093933105\n",
      "CLA KV shared: model.layers.25.self_attn.k_proj.weight 1.0\n",
      "CLA KV shared: model.layers.25.self_attn.v_proj.weight 1.0\n",
      "Trained: model.layers.26.self_attn.k_proj.weight 0.8622622489929199\n",
      "Trained: model.layers.26.self_attn.v_proj.weight 0.7419276237487793\n",
      "CLA KV shared: model.layers.27.self_attn.k_proj.weight 1.0\n",
      "CLA KV shared: model.layers.27.self_attn.v_proj.weight 1.0\n",
      "Trained: model.layers.28.self_attn.k_proj.weight 0.8622450828552246\n",
      "Trained: model.layers.28.self_attn.v_proj.weight 0.7657301425933838\n",
      "CLA KV shared: model.layers.29.self_attn.k_proj.weight 1.0\n",
      "CLA KV shared: model.layers.29.self_attn.v_proj.weight 1.0\n",
      "Trained: model.layers.30.self_attn.k_proj.weight 0.8465535640716553\n",
      "Trained: model.layers.30.self_attn.v_proj.weight 0.806581974029541\n",
      "CLA KV shared: model.layers.31.self_attn.k_proj.weight 1.0\n",
      "CLA KV shared: model.layers.31.self_attn.v_proj.weight 1.0\n"
     ]
    }
   ],
   "source": [
    "cla_factor = 2\n",
    "for name, param in iter(orig_weights.items()):\n",
    "    if \"k_proj\" in name or \"v_proj\" in name:\n",
    "        layer_idx = int(name.split('.')[2])\n",
    "        if layer_idx % cla_factor != 0:\n",
    "            closeness = torch.isclose(CLA2_weights[name], orig_weights[name], rtol=1e-5, atol=1e-5).float().mean().item()\n",
    "            assert closeness == 1.0\n",
    "            print(\"CLA KV shared:\", name, closeness)\n",
    "        else:\n",
    "            closeness = torch.isclose(CLA2_weights[name], orig_weights[name], rtol=1e-5, atol=1e-5).float().mean().item()\n",
    "            assert closeness != 1.0\n",
    "            print(\"Trained:\", name, closeness)\n",
    "    else:\n",
    "        # closeness = torch.isclose(CLA2_weights[name], orig_weights[name], rtol=1e-5, atol=1e-5).float().mean().item()\n",
    "        # print(\"Trained:\", name, closeness)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec2d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90636e-086a-4a79-94e4-3685a3139197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
