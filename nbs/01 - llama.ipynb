{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce74545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformers fork: https://github.com/AnswerDotAI/transformers/tree/cla-llama\n",
    "\n",
    "# vscode jupyter kill hanging process:\n",
    "# ps aux | grep \"/workspace/py_venvs/cla/bin/python -m ipykernel_launcher\" | awk '{print $2}' | xargs kill -9\n",
    "# ps aux | grep \"/workspace/py_venvs/vllm/bin/python -m ipykernel_launcher\" | awk '{print $2}' | xargs kill -9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb54ff83-3439-4f53-a7cb-b0ef203ebff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "779e56bc-c197-4aca-958f-5a031add2793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f16ad8c-9c6d-46ea-b567-3993142788b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.95s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e57448-e083-42ae-9ce4-cab7d662d0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaDecoderLayer(\n",
       "  (self_attn): LlamaSdpaAttention(\n",
       "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
       "  )\n",
       "  (mlp): LlamaMLP(\n",
       "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): LlamaRMSNorm()\n",
       "  (post_attention_layernorm): LlamaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c0a8d1f-da14-467e-b7b4-180e1d7fcedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42890249-c3e1-47c0-a2b9-373555b035d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1579c5cb-5832-4617-977a-15474fc5a31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training, model.config.use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "031466fd-d65a-464e-a4cd-77a0bb768c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0,100,(1,16)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2749af3-bbc8-4082-879f-935df32321ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd82da3f-3cfb-4754-b270-0a9ccad2a9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cla = False\n",
    "model.config.cla_factor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f4a54f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.models.llama.modeling_llama.LlamaDecoderLayer, OrderedDict())"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].__class__, model.model.layers[0]._forward_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d79e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationHook:\n",
    "    def __init__(self):\n",
    "        self.activation = {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.activation = {}\n",
    "    \n",
    "    def get_activation(self, name):\n",
    "        def hook(model, input, output):\n",
    "            # last output in decoder layer is the cla activations.\n",
    "            cla_key_value_states = output[-1]\n",
    "            if isinstance(cla_key_value_states, torch.Tensor):\n",
    "                cla_key_value_states = cla_key_value_states.detach().cpu()\n",
    "            self.activation[name] = cla_key_value_states\n",
    "        return hook\n",
    "\n",
    "    def register_hooks(self, model):\n",
    "        for i, layer in enumerate(model.model.layers):\n",
    "            layer.register_forward_hook(self.get_activation('decoder_layer{}'.format(i)))\n",
    "            \n",
    "    def reset_all_hooks(self, model):\n",
    "        for i, layer in enumerate(model.model.layers):\n",
    "            layer._forward_hooks = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81f6a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_hook = ActivationHook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e278954",
   "metadata": {},
   "source": [
    "### No CLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3864a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_hook.reset_all_hooks(model)\n",
    "activation_hook.register_hooks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a77335bf-5a17-442e-80e8-8784b010a206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "output = model(**{\"input_ids\":x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4177fc4-d277-4517-b479-8ecab080e59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0766, -0.4452, -2.1991,  ..., -3.8720, -3.8722, -3.8722],\n",
       "         [ 5.3667,  7.0156,  5.0217,  ..., -2.2082, -2.2083, -2.2085],\n",
       "         [ 1.2831, -6.6209,  5.4645,  ..., -4.2729, -4.2731, -4.2730],\n",
       "         ...,\n",
       "         [ 7.5572,  7.9917,  8.5733,  ..., -3.0052, -3.0056, -3.0054],\n",
       "         [ 7.6061,  8.6619,  8.5458,  ..., -3.0663, -3.0668, -3.0666],\n",
       "         [ 7.8302,  8.7273,  9.2386,  ..., -3.0484, -3.0487, -3.0485]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "488a64fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(v is None for _,v in activation_hook.activation.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ec7bf6",
   "metadata": {},
   "source": [
    "### CLA with Factor=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b34a13d2-3e79-4d50-b44b-205149d7536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cla = True\n",
    "model.config.cla_factor = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e6d3a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_hook.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b00b82f9-8d2e-4fcd-b912-414cad4b2f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**{\"input_ids\":x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bef5954d-f12b-448b-87b3-76a58711a621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0766, -0.4452, -2.1991,  ..., -3.8720, -3.8722, -3.8722],\n",
       "         [ 5.3667,  7.0156,  5.0217,  ..., -2.2082, -2.2083, -2.2085],\n",
       "         [ 1.2831, -6.6209,  5.4645,  ..., -4.2729, -4.2731, -4.2730],\n",
       "         ...,\n",
       "         [ 7.5572,  7.9917,  8.5733,  ..., -3.0052, -3.0056, -3.0054],\n",
       "         [ 7.6061,  8.6619,  8.5458,  ..., -3.0663, -3.0668, -3.0666],\n",
       "         [ 7.8302,  8.7273,  9.2386,  ..., -3.0484, -3.0487, -3.0485]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50a6fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_states, value_states = activation_hook.activation['decoder_layer0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15140189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 16, 128]), torch.Size([1, 8, 16, 128]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_states.shape, value_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2bcb2a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_key_states, prev_value_states = None, None\n",
    "for layer_name, (key_states, value_states) in activation_hook.activation.items():\n",
    "    if prev_key_states is None:\n",
    "        prev_key_states, prev_value_states = key_states, value_states\n",
    "    else:\n",
    "        assert not torch.equal(prev_key_states, key_states)\n",
    "        assert not torch.equal(prev_value_states, value_states)\n",
    "    \n",
    "        assert prev_key_states.shape == key_states.shape\n",
    "        assert prev_value_states.shape == value_states.shape\n",
    "        \n",
    "        prev_key_states, prev_value_states = key_states, value_states "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc02f65a",
   "metadata": {},
   "source": [
    "### CLA with Factor=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8faf6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cla = True\n",
    "model.config.cla_factor = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5e780b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_hook.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35cad73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**{\"input_ids\":x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7307a413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6762,  4.2007,  5.9417,  ..., -2.3479, -2.3479, -2.3478],\n",
       "         [ 7.2270,  9.1649,  3.4653,  ..., -2.7682, -2.7683, -2.7680],\n",
       "         [ 3.2660,  4.7468,  5.0928,  ..., -3.5554, -3.5555, -3.5554],\n",
       "         ...,\n",
       "         [ 4.1864,  6.8682,  5.4038,  ..., -2.3405, -2.3409, -2.3406],\n",
       "         [ 3.7679,  6.3121,  5.5808,  ..., -3.0443, -3.0442, -3.0441],\n",
       "         [ 3.8976,  6.1718,  5.9425,  ..., -3.0508, -3.0509, -3.0507]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c50866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_states0, value_states0 = activation_hook.activation['decoder_layer0']\n",
    "key_states1, value_states1 = activation_hook.activation['decoder_layer1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d9dd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(key_states0, key_states1)\n",
    "assert torch.equal(value_states0, value_states1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9503e267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 16, 128]), torch.Size([1, 8, 16, 128]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_states.shape, value_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "321de53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_key_states, prev_value_states = None, None\n",
    "prev_cla_group = None\n",
    "for layer_name, (key_states, value_states) in activation_hook.activation.items():\n",
    "    if prev_key_states is None:\n",
    "        prev_key_states, prev_value_states = key_states, value_states\n",
    "        prev_cla_group = int(layer_name.removeprefix('decoder_layer')) // model.config.cla_factor\n",
    "        \n",
    "    else:\n",
    "        curr_cla_group = int(layer_name.removeprefix('decoder_layer')) // model.config.cla_factor\n",
    "        if prev_cla_group == curr_cla_group:\n",
    "            assert torch.equal(prev_key_states, key_states)\n",
    "            assert torch.equal(prev_value_states, value_states)\n",
    "        else:\n",
    "            assert not torch.equal(prev_key_states, key_states)\n",
    "            assert not torch.equal(prev_value_states, value_states)\n",
    "         \n",
    "        assert prev_key_states.shape == key_states.shape\n",
    "        assert prev_value_states.shape == value_states.shape\n",
    "        \n",
    "        prev_key_states, prev_value_states = key_states, value_states \n",
    "        prev_cla_group = curr_cla_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26983b51",
   "metadata": {},
   "source": [
    "### CLA with Factor=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f12fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cla = True\n",
    "model.config.cla_factor = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be6f441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_hook.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dbd9048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**{\"input_ids\":x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "490427fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0454,  4.6148,  6.6658,  ..., -2.3465, -2.3464, -2.3464],\n",
       "         [ 6.3484,  7.1754,  4.7409,  ..., -2.7433, -2.7432, -2.7434],\n",
       "         [ 4.5100,  1.7046,  2.1793,  ..., -1.8608, -1.8610, -1.8609],\n",
       "         ...,\n",
       "         [ 0.0428, -0.0673,  0.7074,  ..., -0.3057, -0.3059, -0.3058],\n",
       "         [ 1.6149,  0.7311,  2.0330,  ..., -0.4262, -0.4264, -0.4264],\n",
       "         [-0.6556,  0.6304,  0.7900,  ..., -0.2798, -0.2799, -0.2799]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc495602",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_states0, value_states0 = activation_hook.activation['decoder_layer0']\n",
    "key_states1, value_states1 = activation_hook.activation['decoder_layer1']\n",
    "key_states2, value_states2 = activation_hook.activation['decoder_layer2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e911bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(key_states0, key_states1)\n",
    "assert torch.equal(value_states0, value_states1)\n",
    "\n",
    "assert torch.equal(key_states1, key_states2)\n",
    "assert torch.equal(value_states1, value_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e7b5caad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 16, 128]), torch.Size([1, 8, 16, 128]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_states.shape, value_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d204c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_key_states, prev_value_states = None, None\n",
    "prev_cla_group = None\n",
    "for layer_name, (key_states, value_states) in activation_hook.activation.items():\n",
    "    if prev_key_states is None:\n",
    "        prev_key_states, prev_value_states = key_states, value_states\n",
    "        prev_cla_group = int(layer_name.removeprefix('decoder_layer')) // model.config.cla_factor\n",
    "    else:\n",
    "        curr_cla_group = int(layer_name.removeprefix('decoder_layer')) // model.config.cla_factor\n",
    "        if prev_cla_group == curr_cla_group:\n",
    "            assert torch.equal(prev_key_states, key_states)\n",
    "            assert torch.equal(prev_value_states, value_states)\n",
    "        else:\n",
    "            assert not torch.equal(prev_key_states, key_states)\n",
    "            assert not torch.equal(prev_value_states, value_states)\n",
    "         \n",
    "        assert prev_key_states.shape == key_states.shape\n",
    "        assert prev_value_states.shape == value_states.shape\n",
    "        \n",
    "        prev_key_states, prev_value_states = key_states, value_states \n",
    "        prev_cla_group = curr_cla_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de5af4-c1eb-47b5-9d64-918a55ab584b",
   "metadata": {},
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d6e7558-0048-4268-9bcb-eff8e34ab59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor(tokenizer.encode(\"Say Hello world\")).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a77f32e0-b352-45b9-8dbf-f41eb133933c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Say Hello world 10 times as numbered list.\"}\n",
    "]\n",
    "input_tokens = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True, \n",
    "    return_tensors=\"pt\"\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "52e5a6b6-6d1b-454c-8f34-23cb9e048cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "57379fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"cla_factor\": 3,\n",
       "  \"eos_token_id\": 128009,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 2.0,\n",
       "    \"type\": \"dynamic\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.43.0.dev0\",\n",
       "  \"use_cache\": false,\n",
       "  \"use_cla\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a265249e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 3)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cla, model.config.cla_factor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d4631fb0-7f79-4e34-b053-2b57ca58e4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "new_tokens = model.generate(input_tokens, max_new_tokens=128, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dc900788-dd24-44f3-b610-3f68bd881d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Say Hello world 10 times as numbered list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "_{{_.__@__ @____,___­__ htt_ htt_ndl_opak_. __ @_zn___/ @__ @__@____ httunny_,&__/_.opak_ @_ htt_undedylvania_ständ_ htt_ryn htt oci_ `_»_-__/__/__ ociortedopro_andalone_ htt_zn_ magneticzn__/znundedusk _ manageable Towersortedômoprzn-andômständ_unded-light-andunded _ Dutryn-and-and-and_oproznoproandalone htt_oratezn_ocationsdevil-and-domopro-dev\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(new_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2d65fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cla = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Say Hello world 10 times as numbered list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here is the list of \"Hello World\" said 10 times:\n",
      "\n",
      "1. Hello World!\n",
      "2. Hello World!\n",
      "3. Hello World!\n",
      "4. Hello World!\n",
      "5. Hello World!\n",
      "6. Hello World!\n",
      "7. Hello World!\n",
      "8. Hello World!\n",
      "9. Hello World!\n",
      "10. Hello World!<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "new_tokens = model.generate(input_tokens, max_new_tokens=128, use_cache=False)\n",
    "print(tokenizer.decode(new_tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f4a9fb-1547-4f99-81d7-5f85b0a63fad",
   "metadata": {},
   "source": [
    "### Weight Prep for XoRA CLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "702f2af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.utils import hub, SAFE_WEIGHTS_NAME, SAFE_WEIGHTS_INDEX_NAME\n",
    "from tqdm import tqdm\n",
    "import safetensors\n",
    "import safetensors.torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60329c2c-4242-4a0d-af09-a5b07c4378a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "idx = hub.cached_file(model_name, SAFE_WEIGHTS_INDEX_NAME)\n",
    "files, _ = hub.get_checkpoint_shard_files(model_name, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3c18b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/model-00001-of-00004.safetensors',\n",
       " '/root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/model-00002-of-00004.safetensors',\n",
       " '/root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/model-00003-of-00004.safetensors',\n",
       " '/root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/model-00004-of-00004.safetensors']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b530ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading & Quantizing Model Shards: 100%|██████████| 4/4 [00:01<00:00,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "all_weights = {}\n",
    "for filename in tqdm(files, desc=\"Loading & Quantizing Model Shards\", disable=0, position=0):\n",
    "\tweights = safetensors.torch.load_file(filename)\n",
    "\tall_weights.update(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "258c4d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 291)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(weights), len(all_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d8c14755",
   "metadata": {},
   "outputs": [],
   "source": [
    "cla_weights = {}\n",
    "cla_factor = 2\n",
    "for name, param in iter(all_weights.items()):\n",
    "    if \"k_proj\" in name or \"v_proj\" in name:\n",
    "        layer_idx = int(name.split('.')[2])\n",
    "        if layer_idx % cla_factor != 0:\n",
    "            cla_group_idx = layer_idx // cla_factor * cla_factor\n",
    "            cla_weights[name] = all_weights[name.replace(f\"model.layers.{layer_idx}\", f\"model.layers.{cla_group_idx}\")].clone()\n",
    "        else:\n",
    "            cla_weights[name] = all_weights[name]\n",
    "    else:\n",
    "        cla_weights[name] = all_weights[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be46a48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cla_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "26f9e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(cla_weights[\"model.layers.0.self_attn.k_proj.weight\"], cla_weights[\"model.layers.1.self_attn.k_proj.weight\"])\n",
    "assert torch.equal(cla_weights[\"model.layers.2.self_attn.k_proj.weight\"], cla_weights[\"model.layers.3.self_attn.k_proj.weight\"])\n",
    "assert not torch.equal(cla_weights[\"model.layers.3.self_attn.k_proj.weight\"], cla_weights[\"model.layers.4.self_attn.k_proj.weight\"])\n",
    "assert torch.equal(cla_weights[\"model.layers.14.self_attn.k_proj.weight\"], cla_weights[\"model.layers.15.self_attn.k_proj.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "23f7022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "safetensors.torch.save_file(cla_weights, \"/workspace/models/meta-llama/Meta-Llama-3-8B-Instruct-cla2.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "34e0b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cla_weights = {}\n",
    "cla_factor = 3\n",
    "for name, param in iter(all_weights.items()):\n",
    "    if \"k_proj\" in name or \"v_proj\" in name:\n",
    "        layer_idx = int(name.split('.')[2])\n",
    "        if layer_idx % cla_factor != 0:\n",
    "            cla_group_idx = layer_idx // cla_factor * cla_factor\n",
    "            cla_weights[name] = all_weights[name.replace(f\"model.layers.{layer_idx}\", f\"model.layers.{cla_group_idx}\")].clone()\n",
    "        else:\n",
    "            cla_weights[name] = all_weights[name]\n",
    "    else:\n",
    "        cla_weights[name] = all_weights[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5a0b538e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cla_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8df4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(cla_weights[\"model.layers.0.self_attn.k_proj.weight\"], cla_weights[\"model.layers.1.self_attn.k_proj.weight\"])\n",
    "assert torch.equal(cla_weights[\"model.layers.1.self_attn.k_proj.weight\"], cla_weights[\"model.layers.2.self_attn.k_proj.weight\"])\n",
    "assert not torch.equal(cla_weights[\"model.layers.2.self_attn.k_proj.weight\"], cla_weights[\"model.layers.3.self_attn.k_proj.weight\"])\n",
    "assert torch.equal(cla_weights[\"model.layers.9.self_attn.k_proj.weight\"], cla_weights[\"model.layers.10.self_attn.k_proj.weight\"])\n",
    "assert torch.equal(cla_weights[\"model.layers.10.self_attn.k_proj.weight\"], cla_weights[\"model.layers.11.self_attn.k_proj.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "36bac4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "safetensors.torch.save_file(cla_weights, \"/workspace/models/meta-llama/Meta-Llama-3-8B-Instruct-cla3.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e93b23a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_weights = safetensors.torch.load_file(\"/workspace/models/meta-llama/Meta-Llama-3-8B-Instruct-cla3.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lm_head.weight', 'model.embed_tokens.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.norm.weight'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14117500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d17e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac883045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148e5c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b4f6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf2934f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a00f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec2d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90636e-086a-4a79-94e4-3685a3139197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
